{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по одному баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 1 дек\n",
    "\n",
    "__Жесткий дедлайн: 4 дек\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в телефонах, поисковой строке или приложении почты. Полученную систему вам нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex), чтобы ей можно было удобно пользоваться, а так же, чтобы убедиться, что все работает как надо. В этот раз вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей предположительно одинаковых по сложности. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы получилось в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а также отчет с подробным описанием техник, которые в применили для создания вашей системы. Не лишним будет также написать и о том, что у вас не получилось и почему.\n",
    "\n",
    "За часть с заданиями можно будет получить до __5__ баллов, за отчет – до __3__ баллов, 2 балл за доп вопросы, если возникнут, если вопросов не возникло, считаем, что 2 балла вы получили "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import email\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400944</th>\n",
       "      <td>schoolcraft-d/deleted_items/639.</td>\n",
       "      <td>Message-ID: &lt;27404034.1075860743039.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124938</th>\n",
       "      <td>germany-c/all_documents/1201.</td>\n",
       "      <td>Message-ID: &lt;671277.1075853689632.JavaMail.eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268219</th>\n",
       "      <td>lay-k/all_documents/113.</td>\n",
       "      <td>Message-ID: &lt;28236766.1075840203510.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261117</th>\n",
       "      <td>kitchen-l/sent_items/1678.</td>\n",
       "      <td>Message-ID: &lt;25262822.1075855292090.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368594</th>\n",
       "      <td>rapp-b/inbox/134.</td>\n",
       "      <td>Message-ID: &lt;1569665.1075859076389.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396743</th>\n",
       "      <td>sanders-r/metals/3.</td>\n",
       "      <td>Message-ID: &lt;140925.1075853240912.JavaMail.eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221631</th>\n",
       "      <td>kaminski-v/sent_items/1255.</td>\n",
       "      <td>Message-ID: &lt;15856294.1075862456029.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228348</th>\n",
       "      <td>kean-s/all_documents/1819.</td>\n",
       "      <td>Message-ID: &lt;7124714.1075846189168.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>bass-e/all_documents/280.</td>\n",
       "      <td>Message-ID: &lt;7395119.1075854581820.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115616</th>\n",
       "      <td>fossum-d/_sent_mail/809.</td>\n",
       "      <td>Message-ID: &lt;21098492.1075842566220.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84097</th>\n",
       "      <td>davis-d/all_documents/398.</td>\n",
       "      <td>Message-ID: &lt;25087428.1075853953738.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319146</th>\n",
       "      <td>mann-k/sent/2018.</td>\n",
       "      <td>Message-ID: &lt;22780967.1075845967109.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349431</th>\n",
       "      <td>nemec-g/inbox/246.</td>\n",
       "      <td>Message-ID: &lt;26736237.1075852330316.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34373</th>\n",
       "      <td>blair-l/meetings/1030.</td>\n",
       "      <td>Message-ID: &lt;1457928.1075853048377.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509944</th>\n",
       "      <td>williams-w3/bill_williams_iii/962.</td>\n",
       "      <td>Message-ID: &lt;13265580.1075863318164.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file  \\\n",
       "400944    schoolcraft-d/deleted_items/639.   \n",
       "124938       germany-c/all_documents/1201.   \n",
       "268219            lay-k/all_documents/113.   \n",
       "261117          kitchen-l/sent_items/1678.   \n",
       "368594                   rapp-b/inbox/134.   \n",
       "396743                 sanders-r/metals/3.   \n",
       "221631         kaminski-v/sent_items/1255.   \n",
       "228348          kean-s/all_documents/1819.   \n",
       "12595            bass-e/all_documents/280.   \n",
       "115616            fossum-d/_sent_mail/809.   \n",
       "84097           davis-d/all_documents/398.   \n",
       "319146                   mann-k/sent/2018.   \n",
       "349431                  nemec-g/inbox/246.   \n",
       "34373               blair-l/meetings/1030.   \n",
       "509944  williams-w3/bill_williams_iii/962.   \n",
       "\n",
       "                                                  message  \n",
       "400944  Message-ID: <27404034.1075860743039.JavaMail.e...  \n",
       "124938  Message-ID: <671277.1075853689632.JavaMail.eva...  \n",
       "268219  Message-ID: <28236766.1075840203510.JavaMail.e...  \n",
       "261117  Message-ID: <25262822.1075855292090.JavaMail.e...  \n",
       "368594  Message-ID: <1569665.1075859076389.JavaMail.ev...  \n",
       "396743  Message-ID: <140925.1075853240912.JavaMail.eva...  \n",
       "221631  Message-ID: <15856294.1075862456029.JavaMail.e...  \n",
       "228348  Message-ID: <7124714.1075846189168.JavaMail.ev...  \n",
       "12595   Message-ID: <7395119.1075854581820.JavaMail.ev...  \n",
       "115616  Message-ID: <21098492.1075842566220.JavaMail.e...  \n",
       "84097   Message-ID: <25087428.1075853953738.JavaMail.e...  \n",
       "319146  Message-ID: <22780967.1075845967109.JavaMail.e...  \n",
       "349431  Message-ID: <26736237.1075852330316.JavaMail.e...  \n",
       "34373   Message-ID: <1457928.1075853048377.JavaMail.ev...  \n",
       "509944  Message-ID: <13265580.1075863318164.JavaMail.e...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Message-ID: <15464986.1075855378456.JavaMail.evans@thyme>\\nDate: Fri, 4 May 2001 13:51:00 -0700 (PDT)\\nFrom: phillip.allen@enron.com\\nTo: john.lavorato@enron.com\\nSubject: Re:\\nMime-Version: 1.0\\nContent-Type: text/plain; charset=us-ascii\\nContent-Transfer-Encoding: 7bit\\nX-From: Phillip K Allen\\nX-To: John J Lavorato <John J Lavorato/ENRON@enronXgate@ENRON>\\nX-cc: \\nX-bcc: \\nX-Folder: \\\\Phillip_Allen_Jan2002_1\\\\Allen, Phillip K.\\\\'Sent Mail\\nX-Origin: Allen-P\\nX-FileName: pallen (Non-Privileged).pst\\n\\nTraveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.iloc[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (1 балл).__ Очистите корпус текстов по вашему усмотрению. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст. Оценка будет выставляться по близости вашего результата к этому идеалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cleaned_bodies(messages):\n",
    "    cleaned_messages = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        try:\n",
    "            # преобразуем сообщение в объект email\n",
    "            email_obj = email.message_from_string(msg)\n",
    "            raw_msg = email_obj.get_payload()  # извлекаем само тело письма\n",
    "            \n",
    "            # удаляем метаинформацию (заголовки, пересылки)\n",
    "            clean_msg = re.split(r'^\\s*[-]+ Forwarded by.*$', raw_msg, flags=re.MULTILINE)[-1]\n",
    "            \n",
    "            # удаляем заголовки (From, To, Date и т.д.)\n",
    "            clean_msg = re.sub(r\"(?im)^(from:|to:|sent:|subject:|date:|cc:|bcc:|x-.*:).*\", \"\", clean_msg)\n",
    "            \n",
    "            # удаляем временные отметки и избыточные фрагменты заголовков в начале текста\n",
    "            clean_msg = re.sub(r\"(?im)^\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*.*\\n\", \" \", clean_msg)\n",
    "            \n",
    "            # убираем текстовые метки времени и пересылок, остающиеся в середине письма\n",
    "            clean_msg = re.sub(r\"(?im)^\\d{4,8}\\s+(AM|PM)\\s*.*$\", \"\", clean_msg, flags=re.MULTILINE)\n",
    "            \n",
    "            # очищаем от специальных символов (оставляем буквы, цифры, пробелы, точки и запятые)\n",
    "            clean_msg = re.sub(r'[^A-Za-z0-9 ,.\\n]', '', clean_msg)\n",
    "            \n",
    "            # убираем табуляции и множественные переводы строк\n",
    "            clean_msg = re.sub(r'\\t', ' ', clean_msg)\n",
    "            clean_msg = re.sub(r'\\n+', ' ', clean_msg)\n",
    "            \n",
    "            # удаляем лишние пробелы\n",
    "            clean_msg = re.sub(r' {2,}', ' ', clean_msg).strip()\n",
    "            \n",
    "            cleaned_messages.append(clean_msg)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # если что-то пошло не так, выводим сообщение об ошибке\n",
    "            cleaned_messages.append(f\"Error processing message: {e}\")\n",
    "    \n",
    "    return cleaned_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 53s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "      <td>test successful. way to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "      <td>Lets shoot for Tuesday at 1145.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0     allen-p/_sent_mail/1.   \n",
       "1    allen-p/_sent_mail/10.   \n",
       "2   allen-p/_sent_mail/100.   \n",
       "3  allen-p/_sent_mail/1000.   \n",
       "4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                             message  \\\n",
       "0  Message-ID: <18782981.1075855378110.JavaMail.e...   \n",
       "1  Message-ID: <15464986.1075855378456.JavaMail.e...   \n",
       "2  Message-ID: <24216240.1075855687451.JavaMail.e...   \n",
       "3  Message-ID: <13505866.1075863688222.JavaMail.e...   \n",
       "4  Message-ID: <30922949.1075863688243.JavaMail.e...   \n",
       "\n",
       "                                                body  \n",
       "0                               Here is our forecast  \n",
       "1  Traveling to have a business meeting takes the...  \n",
       "2                         test successful. way to go  \n",
       "3  Randy, Can you send me a schedule of the salar...  \n",
       "4                    Lets shoot for Tuesday at 1145.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "messages_body = extract_cleaned_bodies(emails['message'])\n",
    "emails['body'] = messages_body\n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traveling to have a business meeting takes the fun out of the trip. Especially if you have to prepare a presentation. I would suggest holding the business plan meetings here then take a trip without any formal business meetings. I would even try and get some honest opinions on whether a trip is even desired or necessary. As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not. Too often the presenter speaks and the others are quiet just waiting for their turn. The meetings might be better if held in a round table discussion format. My suggestion for where to go is Austin. Play golf and rent a ski boat and jet skis. Flying somewhere takes too much time.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.iloc[1][2] # посмотрим на результат обработки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы считаете, что результат получается лучше без нее, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenization(text):\n",
    "    text = text.lower() # приводим сначала текст к одному регистру\n",
    "    \n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text) # будем разбивать текст на слова и сохраним пунктуацию\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "emails_token = [text_tokenization(em) for em in emails['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['traveling',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'business',\n",
       " 'meeting',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'fun',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'trip',\n",
       " '.',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'prepare',\n",
       " 'a',\n",
       " 'presentation',\n",
       " '.',\n",
       " 'i',\n",
       " 'would',\n",
       " 'suggest',\n",
       " 'holding',\n",
       " 'the',\n",
       " 'business',\n",
       " 'plan',\n",
       " 'meetings',\n",
       " 'here',\n",
       " 'then',\n",
       " 'take',\n",
       " 'a',\n",
       " 'trip',\n",
       " 'without',\n",
       " 'any',\n",
       " 'formal',\n",
       " 'business',\n",
       " 'meetings',\n",
       " '.',\n",
       " 'i',\n",
       " 'would',\n",
       " 'even',\n",
       " 'try',\n",
       " 'and',\n",
       " 'get',\n",
       " 'some',\n",
       " 'honest',\n",
       " 'opinions',\n",
       " 'on',\n",
       " 'whether',\n",
       " 'a',\n",
       " 'trip',\n",
       " 'is',\n",
       " 'even',\n",
       " 'desired',\n",
       " 'or',\n",
       " 'necessary',\n",
       " '.',\n",
       " 'as',\n",
       " 'far',\n",
       " 'as',\n",
       " 'the',\n",
       " 'business',\n",
       " 'meetings',\n",
       " ',',\n",
       " 'i',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'more',\n",
       " 'productive',\n",
       " 'to',\n",
       " 'try',\n",
       " 'and',\n",
       " 'stimulate',\n",
       " 'discussions',\n",
       " 'across',\n",
       " 'the',\n",
       " 'different',\n",
       " 'groups',\n",
       " 'about',\n",
       " 'what',\n",
       " 'is',\n",
       " 'working',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'not',\n",
       " '.',\n",
       " 'too',\n",
       " 'often',\n",
       " 'the',\n",
       " 'presenter',\n",
       " 'speaks',\n",
       " 'and',\n",
       " 'the',\n",
       " 'others',\n",
       " 'are',\n",
       " 'quiet',\n",
       " 'just',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'their',\n",
       " 'turn',\n",
       " '.',\n",
       " 'the',\n",
       " 'meetings',\n",
       " 'might',\n",
       " 'be',\n",
       " 'better',\n",
       " 'if',\n",
       " 'held',\n",
       " 'in',\n",
       " 'a',\n",
       " 'round',\n",
       " 'table',\n",
       " 'discussion',\n",
       " 'format',\n",
       " '.',\n",
       " 'my',\n",
       " 'suggestion',\n",
       " 'for',\n",
       " 'where',\n",
       " 'to',\n",
       " 'go',\n",
       " 'is',\n",
       " 'austin',\n",
       " '.',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'and',\n",
       " 'rent',\n",
       " 'a',\n",
       " 'ski',\n",
       " 'boat',\n",
       " 'and',\n",
       " 'jet',\n",
       " 'skis',\n",
       " '.',\n",
       " 'flying',\n",
       " 'somewhere',\n",
       " 'takes',\n",
       " 'too',\n",
       " 'much',\n",
       " 'time',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_token[1] # помсотрим на нашем примере, как разбилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "Описанная система будет состоять из двух частей: дополнение слова до целого и генерация продолжения текста (или вариантов продолжений). Начнем с первой части.\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова за $O(n + m)$, где $m$ – число подходящих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл).__ Допишите префиксное дерево для поиска слов по префиксу. Ваше дерево должно работать за $O(n + m)$ операции, в противном случае вы не получите баллов за это задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        # словарь с буквами, которые могут идти после данной вершины\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()  # корень дерева\n",
    "        for word in vocabulary:\n",
    "            self.add_word(word)  # добавляем каждое слово в дерево\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        current = self.root  # начинаем с корня\n",
    "        for w in word:\n",
    "            if w not in current.children:\n",
    "                current.children[w] = PrefixTreeNode()  # создаем новый узел, если его нет\n",
    "            current = current.children[w]\n",
    "        current.is_end_of_word = True  # отмечаем конец слова\n",
    "\n",
    "    def search_prefix(self, prefix: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "        # рекурсивная функция для поиска всех листьев\n",
    "        def _find_all_leaves(node, current_word, result):\n",
    "            if node.is_end_of_word:\n",
    "                result.append(current_word)\n",
    "\n",
    "            for c, child in node.children.items():\n",
    "                _find_all_leaves(child, current_word + c, result)\n",
    "\n",
    "        # ищем узел, соответствующий последнему символу префикса\n",
    "        start_node = self.root\n",
    "        for p in prefix:\n",
    "            if p not in start_node.children:\n",
    "                return []  # значит префикс не найден\n",
    "            start_node = start_node.children[p]\n",
    "\n",
    "        # собираем все слова, начиная с узла start_node\n",
    "        result = []\n",
    "        _find_all_leaves(start_node, prefix, result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его встречаемости в корпусе.\n",
    "\n",
    "__Задание 3 (1 балл).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.word_counter = 0\n",
    "        \n",
    "        # считаем частоты слов и общее количество слов\n",
    "        for text in corpus:\n",
    "            for word in text:\n",
    "                self.word_counts[word] += 1\n",
    "                self.word_counter += 1\n",
    "\n",
    "        # формируем словарь уникальных слов\n",
    "        vocabulary_words = list(self.word_counts.keys())\n",
    "        # ну и в итоге создаем префиксное дерево\n",
    "        self.prefix_tree = PrefixTree(vocabulary_words)\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words, probs = [], []\n",
    "        words = self.prefix_tree.search_prefix(prefix)\n",
    "        probs = [self.word_counts[word] / self.word_counter for word in words]\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему несколько следующих слов с учетом дописанного. Для этого мы воспользуемся n-граммами и будем советовать n следующих слов. Но сперва нужно получить n-граммную модель.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "Тогда, нам нужно оценить $P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (1 балл).__ Напишите класс для n-граммной модели. Понятное дело, никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus: List[List[str]], n: int):\n",
    "        self.n = n\n",
    "        self.count_ngrams = defaultdict(int)  # сначала сдеалем словарь для подсчета n-грамм\n",
    "        self.count_context = defaultdict(int)  # похожий создадим и для подсчета контекстов\n",
    "\n",
    "        # перейдем к созданию модели n-грамм:\n",
    "        for c in corpus:\n",
    "            sentence_len = len(c)\n",
    "            for i in range(sentence_len):\n",
    "                for j in range(1, sentence_len - i + 1):  # перебираем все возможные длины n-грамм\n",
    "                    ngram = tuple(c[i:i + j])  # создаем n-грамму\n",
    "                    self.count_ngrams[ngram] += 1\n",
    "                    if len(ngram) > 1:  # контекст существует только для n-грамм длиной > 1\n",
    "                        context = ngram[:-1]  # контекстом будут считаться все элементы, кроме последнего\n",
    "                        self.count_context[context] += 1\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: List[str]) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после префикса,\n",
    "        а также их вероятности.\n",
    "        \"\"\"\n",
    "        prefix_tuple = tuple(prefix)  # преобразуем префикс в кортеж для поиска в словарях\n",
    "        total_context_count = self.count_context.get(prefix_tuple, 0)\n",
    "\n",
    "        # если контекст не найден, то возвращаем пустые списки\n",
    "        if total_context_count == 0:\n",
    "            return [], []\n",
    "\n",
    "        # собираем возможные продолжения для следующего слова\n",
    "        continuation_candidates = {\n",
    "            ngram[-1]: self.count_ngrams[ngram]\n",
    "            for ngram in self.count_ngrams\n",
    "            if len(ngram) == len(prefix_tuple) + 1 and ngram[:-1] == prefix_tuple}\n",
    "\n",
    "        # рассчитываем вероятности для каждого следующего слова\n",
    "        next_words = list(continuation_candidates.keys())\n",
    "        probabilities = [count / total_context_count for count in continuation_candidates.values()]\n",
    "\n",
    "        return next_words, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (1 балл).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. Если вы справитесь, то сможете можете добавить опцию поддержки нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text: Union[str, list], n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[str]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        # преобразуем входной текст в список слов\n",
    "        words = text.strip().split() if isinstance(text, str) else text[:]\n",
    "        \n",
    "        if not words:\n",
    "            return []\n",
    "\n",
    "        # после этого получаем дополнение к последнему слову\n",
    "        last_word = words[-1]\n",
    "        complets, probabilities = self.word_completor.get_words_and_probs(last_word)\n",
    "        \n",
    "        completed_word = complets[probabilities.index(max(probabilities))] if complets else last_word\n",
    "        \n",
    "        # далее составляем полный список слов для генерации продолжений\n",
    "        updated_words = words[:-1] + [completed_word]\n",
    "        generated_text = [completed_word]\n",
    "        \n",
    "        n = self.n_gram_model.n\n",
    "        context = updated_words[-(n - 1):] if n > 1 else []\n",
    "\n",
    "        # генерация продолжений текста\n",
    "        for _ in range(n_words):\n",
    "            next_candidates, next_probabilities = self.n_gram_model.get_next_words_and_probs(context)\n",
    "            if not next_candidates:\n",
    "                break\n",
    "            \n",
    "            best_word = next_candidates[next_probabilities.index(max(next_probabilities))]\n",
    "            generated_text.append(best_word)\n",
    "            \n",
    "            # обновляем контекст\n",
    "            context = context[1:] + [best_word] if n > 1 else [best_word]\n",
    "        \n",
    "        suggestions.append(generated_text)\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated suggestion: [['ab', 'bba', 'bbb']]\n"
     ]
    }
   ],
   "source": [
    "# проверка результата выполнения\n",
    "result = text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1)\n",
    "print(f\"Generated suggestion: {result}\")  # выводим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время довести вашу систему до ума. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным, __кроме использования дополнительных данных__. Главное – вы должны обернуть вашу систему в пользовательский интерфейс с помощью [reflex](https://github.com/reflex-dev/reflex). В нем можно реализовать почти любой функционал по вашему желанию.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать.\n",
    "\n",
    "При сдаче решения прикрепите весь ваш __код__, __отчет__ по второй части и __видео__ с демонстрацией работы вашей системы. Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
